{"ast":null,"code":"'use strict';\n\nconst pMap = require('p-map');\n\nconst pDoWhilst = require('p-do-whilst');\n\nconst Entry = require('./entry');\n\nconst hasItems = arr => arr && arr.length > 0;\n\nclass EntryIO {\n  // Fetch log graphs in parallel\n  static async fetchParallel(ipfs, hashes, {\n    length,\n    exclude = [],\n    timeout,\n    concurrency,\n    onProgressCallback\n  }) {\n    const fetchOne = async hash => EntryIO.fetchAll(ipfs, hash, {\n      length,\n      exclude,\n      timeout,\n      onProgressCallback,\n      concurrency\n    });\n\n    const concatArrays = (arr1, arr2) => arr1.concat(arr2);\n\n    const flatten = arr => arr.reduce(concatArrays, []);\n\n    const res = await pMap(hashes, fetchOne, {\n      concurrency: Math.max(concurrency || hashes.length, 1)\n    });\n    return flatten(res);\n  }\n  /**\n   * Fetch log entries\n   *\n   * @param {IPFS} [ipfs] An IPFS instance\n   * @param {string} [hash] Multihash of the entry to fetch\n   * @param {string} [parent] Parent of the node to be fetched\n   * @param {Object} [all] Entries to skip\n   * @param {Number} [amount=-1] How many entries to fetch\n   * @param {Number} [depth=0] Current depth of the recursion\n   * @param {function(hash, entry, parent, depth)} onProgressCallback\n   * @returns {Promise<Array<Entry>>}\n   */\n\n\n  static async fetchAll(ipfs, hashes, {\n    length = -1,\n    exclude = [],\n    timeout,\n    onProgressCallback,\n    onStartProgressCallback,\n    concurrency = 32,\n    delay = 0\n  } = {}) {\n    let result = [];\n    let cache = {};\n    let loadingCache = {};\n    let loadingQueue = Array.isArray(hashes) ? {\n      0: hashes.slice()\n    } : {\n      0: [hashes]\n    };\n    let running = 0; // keep track of how many entries are being fetched at any time\n\n    let maxClock = 0; // keep track of the latest clock time during load\n\n    let minClock = 0; // keep track of the minimum clock time during load\n    // Does the loading queue have more to process?\n\n    const loadingQueueHasMore = () => Object.values(loadingQueue).find(hasItems) !== undefined; // Add a multihash to the loading queue\n\n\n    const addToLoadingQueue = (e, idx) => {\n      if (!loadingCache[e]) {\n        if (!loadingQueue[idx]) loadingQueue[idx] = [];\n\n        if (!loadingQueue[idx].includes(e)) {\n          loadingQueue[idx].push(e);\n        }\n\n        loadingCache[e] = true;\n      }\n    }; // Get the next items to process from the loading queue\n\n\n    const getNextFromQueue = (length = 1) => {\n      const getNext = (res, key, idx) => {\n        const nextItems = loadingQueue[key];\n\n        while (nextItems.length > 0 && res.length < length) {\n          const hash = nextItems.shift();\n          res.push(hash);\n        }\n\n        if (nextItems.length === 0) {\n          delete loadingQueue[key];\n        }\n\n        return res;\n      };\n\n      return Object.keys(loadingQueue).reduce(getNext, []);\n    }; // Add entries that we don't need to fetch to the \"cache\"\n\n\n    const addToExcludeCache = e => {\n      cache[e.hash] = true;\n    }; // Fetch one entry and add it to the results\n\n\n    const fetchEntry = async hash => {\n      if (!hash || cache[hash]) {\n        return;\n      }\n\n      return new Promise(async (resolve, reject) => {\n        // Resolve the promise after a timeout (if given) in order to\n        // not get stuck loading a block that is unreachable\n        const timer = timeout && timeout > 0 ? setTimeout(() => {\n          console.warn(`Warning: Couldn't fetch entry '${hash}', request timed out (${timeout}ms)`);\n          resolve();\n        }, timeout) : null;\n\n        const addToResults = entry => {\n          if (Entry.isEntry(entry)) {\n            const ts = entry.clock.time; // Update min/max clocks\n\n            maxClock = Math.max(maxClock, ts);\n            minClock = result.length > 0 ? Math.min(result[result.length - 1].clock.time, minClock) : maxClock;\n            const isLater = result.length >= length && ts >= minClock;\n\n            const calculateIndex = idx => maxClock - ts + (idx + 1) * idx; // Add the entry to the results if\n            // 1) we're fetching all entries\n            // 2) results is not filled yet\n            // the clock of the entry is later than current known minimum clock time\n\n\n            if (length < 0 || result.length < length || isLater) {\n              result.push(entry);\n              cache[hash] = true;\n\n              if (onProgressCallback) {\n                onProgressCallback(hash, entry, result.length, result.length);\n              }\n            }\n\n            if (length < 0) {\n              // If we're fetching all entries (length === -1), adds nexts and refs to the queue\n              entry.next.forEach(addToLoadingQueue);\n              if (entry.refs) entry.refs.forEach(addToLoadingQueue);\n            } else {\n              // If we're fetching entries up to certain length,\n              // fetch the next if result is filled up, to make sure we \"check\"\n              // the next entry if its clock is later than what we have in the result\n              if (result.length < length || ts > minClock || ts === minClock && !cache[entry.hash]) {\n                entry.next.forEach(e => addToLoadingQueue(e, calculateIndex(0)));\n              }\n\n              if (entry.refs && result.length + entry.refs.length <= length) {\n                entry.refs.forEach((e, i) => addToLoadingQueue(e, calculateIndex(i)));\n              }\n            }\n          }\n        };\n\n        if (onStartProgressCallback) {\n          onStartProgressCallback(hash, null, 0, result.length);\n        }\n\n        try {\n          // Load the entry\n          const entry = await Entry.fromMultihash(ipfs, hash); // Add it to the results\n\n          addToResults(entry); // Simulate network latency (for debugging purposes)\n\n          if (delay > 0) {\n            const sleep = (ms = 0) => new Promise(resolve => setTimeout(resolve, ms));\n\n            await sleep(delay);\n          }\n\n          resolve();\n        } catch (e) {\n          reject(e);\n        } finally {\n          clearTimeout(timer);\n        }\n      });\n    }; // One loop of processing the loading queue\n\n\n    const _processQueue = async () => {\n      if (running < concurrency) {\n        const nexts = getNextFromQueue(concurrency);\n        running += nexts.length;\n        await pMap(nexts, fetchEntry);\n        running -= nexts.length;\n      }\n    }; // Add entries to exclude from processing to the cache before we start\n\n\n    exclude.forEach(addToExcludeCache); // Fetch entries\n\n    await pDoWhilst(_processQueue, loadingQueueHasMore);\n    return result;\n  }\n\n}\n\nmodule.exports = EntryIO;","map":{"version":3,"sources":["/home/dekan/Projects/raid-guild/dao-badges-web/node_modules/ipfs-log/src/entry-io.js"],"names":["pMap","require","pDoWhilst","Entry","hasItems","arr","length","EntryIO","fetchParallel","ipfs","hashes","exclude","timeout","concurrency","onProgressCallback","fetchOne","hash","fetchAll","concatArrays","arr1","arr2","concat","flatten","reduce","res","Math","max","onStartProgressCallback","delay","result","cache","loadingCache","loadingQueue","Array","isArray","slice","running","maxClock","minClock","loadingQueueHasMore","Object","values","find","undefined","addToLoadingQueue","e","idx","includes","push","getNextFromQueue","getNext","key","nextItems","shift","keys","addToExcludeCache","fetchEntry","Promise","resolve","reject","timer","setTimeout","console","warn","addToResults","entry","isEntry","ts","clock","time","min","isLater","calculateIndex","next","forEach","refs","i","fromMultihash","sleep","ms","clearTimeout","_processQueue","nexts","module","exports"],"mappings":"AAAA;;AAEA,MAAMA,IAAI,GAAGC,OAAO,CAAC,OAAD,CAApB;;AACA,MAAMC,SAAS,GAAGD,OAAO,CAAC,aAAD,CAAzB;;AACA,MAAME,KAAK,GAAGF,OAAO,CAAC,SAAD,CAArB;;AAEA,MAAMG,QAAQ,GAAGC,GAAG,IAAIA,GAAG,IAAIA,GAAG,CAACC,MAAJ,GAAa,CAA5C;;AAEA,MAAMC,OAAN,CAAc;AACZ;AACA,eAAaC,aAAb,CAA4BC,IAA5B,EAAkCC,MAAlC,EAA0C;AAAEJ,IAAAA,MAAF;AAAUK,IAAAA,OAAO,GAAG,EAApB;AAAwBC,IAAAA,OAAxB;AAAiCC,IAAAA,WAAjC;AAA8CC,IAAAA;AAA9C,GAA1C,EAA8G;AAC5G,UAAMC,QAAQ,GAAG,MAAOC,IAAP,IAAgBT,OAAO,CAACU,QAAR,CAAiBR,IAAjB,EAAuBO,IAAvB,EAA6B;AAAEV,MAAAA,MAAF;AAAUK,MAAAA,OAAV;AAAmBC,MAAAA,OAAnB;AAA4BE,MAAAA,kBAA5B;AAAgDD,MAAAA;AAAhD,KAA7B,CAAjC;;AACA,UAAMK,YAAY,GAAG,CAACC,IAAD,EAAOC,IAAP,KAAgBD,IAAI,CAACE,MAAL,CAAYD,IAAZ,CAArC;;AACA,UAAME,OAAO,GAAIjB,GAAD,IAASA,GAAG,CAACkB,MAAJ,CAAWL,YAAX,EAAyB,EAAzB,CAAzB;;AACA,UAAMM,GAAG,GAAG,MAAMxB,IAAI,CAACU,MAAD,EAASK,QAAT,EAAmB;AAAEF,MAAAA,WAAW,EAAEY,IAAI,CAACC,GAAL,CAASb,WAAW,IAAIH,MAAM,CAACJ,MAA/B,EAAuC,CAAvC;AAAf,KAAnB,CAAtB;AACA,WAAOgB,OAAO,CAACE,GAAD,CAAd;AACD;AAED;;;;;;;;;;;;;;AAYA,eAAaP,QAAb,CAAuBR,IAAvB,EAA6BC,MAA7B,EAAqC;AAAEJ,IAAAA,MAAM,GAAG,CAAC,CAAZ;AAAeK,IAAAA,OAAO,GAAG,EAAzB;AAA6BC,IAAAA,OAA7B;AAAsCE,IAAAA,kBAAtC;AAA0Da,IAAAA,uBAA1D;AAAmFd,IAAAA,WAAW,GAAG,EAAjG;AAAqGe,IAAAA,KAAK,GAAG;AAA7G,MAAmH,EAAxJ,EAA4J;AAC1J,QAAIC,MAAM,GAAG,EAAb;AACA,QAAIC,KAAK,GAAG,EAAZ;AACA,QAAIC,YAAY,GAAG,EAAnB;AACA,QAAIC,YAAY,GAAGC,KAAK,CAACC,OAAN,CAAcxB,MAAd,IACf;AAAE,SAAGA,MAAM,CAACyB,KAAP;AAAL,KADe,GAEf;AAAE,SAAG,CAACzB,MAAD;AAAL,KAFJ;AAGA,QAAI0B,OAAO,GAAG,CAAd,CAP0J,CAO1I;;AAChB,QAAIC,QAAQ,GAAG,CAAf,CAR0J,CAQzI;;AACjB,QAAIC,QAAQ,GAAG,CAAf,CAT0J,CASzI;AAEjB;;AACA,UAAMC,mBAAmB,GAAG,MAAMC,MAAM,CAACC,MAAP,CAAcT,YAAd,EAA4BU,IAA5B,CAAiCtC,QAAjC,MAA+CuC,SAAjF,CAZ0J,CAc1J;;;AACA,UAAMC,iBAAiB,GAAG,CAACC,CAAD,EAAIC,GAAJ,KAAY;AACpC,UAAI,CAACf,YAAY,CAACc,CAAD,CAAjB,EAAsB;AACpB,YAAI,CAACb,YAAY,CAACc,GAAD,CAAjB,EAAwBd,YAAY,CAACc,GAAD,CAAZ,GAAoB,EAApB;;AACxB,YAAI,CAACd,YAAY,CAACc,GAAD,CAAZ,CAAkBC,QAAlB,CAA2BF,CAA3B,CAAL,EAAoC;AAClCb,UAAAA,YAAY,CAACc,GAAD,CAAZ,CAAkBE,IAAlB,CAAuBH,CAAvB;AACD;;AACDd,QAAAA,YAAY,CAACc,CAAD,CAAZ,GAAkB,IAAlB;AACD;AACF,KARD,CAf0J,CAyB1J;;;AACA,UAAMI,gBAAgB,GAAG,CAAC3C,MAAM,GAAG,CAAV,KAAgB;AACvC,YAAM4C,OAAO,GAAG,CAAC1B,GAAD,EAAM2B,GAAN,EAAWL,GAAX,KAAmB;AACjC,cAAMM,SAAS,GAAGpB,YAAY,CAACmB,GAAD,CAA9B;;AACA,eAAOC,SAAS,CAAC9C,MAAV,GAAmB,CAAnB,IAAwBkB,GAAG,CAAClB,MAAJ,GAAaA,MAA5C,EAAoD;AAClD,gBAAMU,IAAI,GAAGoC,SAAS,CAACC,KAAV,EAAb;AACA7B,UAAAA,GAAG,CAACwB,IAAJ,CAAShC,IAAT;AACD;;AACD,YAAIoC,SAAS,CAAC9C,MAAV,KAAqB,CAAzB,EAA4B;AAC1B,iBAAO0B,YAAY,CAACmB,GAAD,CAAnB;AACD;;AACD,eAAO3B,GAAP;AACD,OAVD;;AAWA,aAAOgB,MAAM,CAACc,IAAP,CAAYtB,YAAZ,EAA0BT,MAA1B,CAAiC2B,OAAjC,EAA0C,EAA1C,CAAP;AACD,KAbD,CA1B0J,CAyC1J;;;AACA,UAAMK,iBAAiB,GAAGV,CAAC,IAAI;AAAEf,MAAAA,KAAK,CAACe,CAAC,CAAC7B,IAAH,CAAL,GAAgB,IAAhB;AAAsB,KAAvD,CA1C0J,CA4C1J;;;AACA,UAAMwC,UAAU,GAAG,MAAOxC,IAAP,IAAgB;AACjC,UAAI,CAACA,IAAD,IAASc,KAAK,CAACd,IAAD,CAAlB,EAA0B;AACxB;AACD;;AAED,aAAO,IAAIyC,OAAJ,CAAY,OAAOC,OAAP,EAAgBC,MAAhB,KAA2B;AAC5C;AACA;AACA,cAAMC,KAAK,GAAGhD,OAAO,IAAIA,OAAO,GAAG,CAArB,GACViD,UAAU,CAAC,MAAM;AACjBC,UAAAA,OAAO,CAACC,IAAR,CAAc,kCAAiC/C,IAAK,yBAAwBJ,OAAQ,KAApF;AACA8C,UAAAA,OAAO;AACR,SAHW,EAGT9C,OAHS,CADA,GAKV,IALJ;;AAOA,cAAMoD,YAAY,GAAIC,KAAD,IAAW;AAC9B,cAAI9D,KAAK,CAAC+D,OAAN,CAAcD,KAAd,CAAJ,EAA0B;AACxB,kBAAME,EAAE,GAAGF,KAAK,CAACG,KAAN,CAAYC,IAAvB,CADwB,CAGxB;;AACAhC,YAAAA,QAAQ,GAAGZ,IAAI,CAACC,GAAL,CAASW,QAAT,EAAmB8B,EAAnB,CAAX;AACA7B,YAAAA,QAAQ,GAAGT,MAAM,CAACvB,MAAP,GAAgB,CAAhB,GACPmB,IAAI,CAAC6C,GAAL,CAASzC,MAAM,CAACA,MAAM,CAACvB,MAAP,GAAgB,CAAjB,CAAN,CAA0B8D,KAA1B,CAAgCC,IAAzC,EAA+C/B,QAA/C,CADO,GAEPD,QAFJ;AAIA,kBAAMkC,OAAO,GAAI1C,MAAM,CAACvB,MAAP,IAAiBA,MAAjB,IAA2B6D,EAAE,IAAI7B,QAAlD;;AACA,kBAAMkC,cAAc,GAAI1B,GAAD,IAAST,QAAQ,GAAG8B,EAAX,GAAiB,CAACrB,GAAG,GAAG,CAAP,IAAYA,GAA7D,CAVwB,CAYxB;AACA;AACA;AACA;;;AACA,gBAAIxC,MAAM,GAAG,CAAT,IAAcuB,MAAM,CAACvB,MAAP,GAAgBA,MAA9B,IAAwCiE,OAA5C,EAAqD;AACnD1C,cAAAA,MAAM,CAACmB,IAAP,CAAYiB,KAAZ;AACAnC,cAAAA,KAAK,CAACd,IAAD,CAAL,GAAc,IAAd;;AAEA,kBAAIF,kBAAJ,EAAwB;AACtBA,gBAAAA,kBAAkB,CAACE,IAAD,EAAOiD,KAAP,EAAcpC,MAAM,CAACvB,MAArB,EAA6BuB,MAAM,CAACvB,MAApC,CAAlB;AACD;AACF;;AAED,gBAAIA,MAAM,GAAG,CAAb,EAAgB;AACd;AACA2D,cAAAA,KAAK,CAACQ,IAAN,CAAWC,OAAX,CAAmB9B,iBAAnB;AACA,kBAAIqB,KAAK,CAACU,IAAV,EAAgBV,KAAK,CAACU,IAAN,CAAWD,OAAX,CAAmB9B,iBAAnB;AACjB,aAJD,MAIO;AACL;AACA;AACA;AACA,kBAAIf,MAAM,CAACvB,MAAP,GAAgBA,MAAhB,IAA0B6D,EAAE,GAAG7B,QAA/B,IAA4C6B,EAAE,KAAK7B,QAAP,IAAmB,CAACR,KAAK,CAACmC,KAAK,CAACjD,IAAP,CAAzE,EAAwF;AACtFiD,gBAAAA,KAAK,CAACQ,IAAN,CAAWC,OAAX,CAAmB7B,CAAC,IAAID,iBAAiB,CAACC,CAAD,EAAI2B,cAAc,CAAC,CAAD,CAAlB,CAAzC;AACD;;AACD,kBAAIP,KAAK,CAACU,IAAN,IAAe9C,MAAM,CAACvB,MAAP,GAAgB2D,KAAK,CAACU,IAAN,CAAWrE,MAA3B,IAAqCA,MAAxD,EAAiE;AAC/D2D,gBAAAA,KAAK,CAACU,IAAN,CAAWD,OAAX,CAAmB,CAAC7B,CAAD,EAAI+B,CAAJ,KAAUhC,iBAAiB,CAACC,CAAD,EAAI2B,cAAc,CAACI,CAAD,CAAlB,CAA9C;AACD;AACF;AACF;AACF,SA1CD;;AA4CA,YAAIjD,uBAAJ,EAA6B;AAC3BA,UAAAA,uBAAuB,CAACX,IAAD,EAAO,IAAP,EAAa,CAAb,EAAgBa,MAAM,CAACvB,MAAvB,CAAvB;AACD;;AAED,YAAI;AACF;AACA,gBAAM2D,KAAK,GAAG,MAAM9D,KAAK,CAAC0E,aAAN,CAAoBpE,IAApB,EAA0BO,IAA1B,CAApB,CAFE,CAIF;;AACAgD,UAAAA,YAAY,CAACC,KAAD,CAAZ,CALE,CAOF;;AACA,cAAIrC,KAAK,GAAG,CAAZ,EAAe;AACb,kBAAMkD,KAAK,GAAG,CAACC,EAAE,GAAG,CAAN,KAAY,IAAItB,OAAJ,CAAYC,OAAO,IAAIG,UAAU,CAACH,OAAD,EAAUqB,EAAV,CAAjC,CAA1B;;AACA,kBAAMD,KAAK,CAAClD,KAAD,CAAX;AACD;;AACD8B,UAAAA,OAAO;AACR,SAbD,CAaE,OAAOb,CAAP,EAAU;AACVc,UAAAA,MAAM,CAACd,CAAD,CAAN;AACD,SAfD,SAeU;AACRmC,UAAAA,YAAY,CAACpB,KAAD,CAAZ;AACD;AACF,OA5EM,CAAP;AA6ED,KAlFD,CA7C0J,CAiI1J;;;AACA,UAAMqB,aAAa,GAAG,YAAY;AAChC,UAAI7C,OAAO,GAAGvB,WAAd,EAA2B;AACzB,cAAMqE,KAAK,GAAGjC,gBAAgB,CAACpC,WAAD,CAA9B;AACAuB,QAAAA,OAAO,IAAI8C,KAAK,CAAC5E,MAAjB;AACA,cAAMN,IAAI,CAACkF,KAAD,EAAQ1B,UAAR,CAAV;AACApB,QAAAA,OAAO,IAAI8C,KAAK,CAAC5E,MAAjB;AACD;AACF,KAPD,CAlI0J,CA2I1J;;;AACAK,IAAAA,OAAO,CAAC+D,OAAR,CAAgBnB,iBAAhB,EA5I0J,CA8I1J;;AACA,UAAMrD,SAAS,CAAC+E,aAAD,EAAgB1C,mBAAhB,CAAf;AAEA,WAAOV,MAAP;AACD;;AAxKW;;AA2KdsD,MAAM,CAACC,OAAP,GAAiB7E,OAAjB","sourcesContent":["'use strict'\n\nconst pMap = require('p-map')\nconst pDoWhilst = require('p-do-whilst')\nconst Entry = require('./entry')\n\nconst hasItems = arr => arr && arr.length > 0\n\nclass EntryIO {\n  // Fetch log graphs in parallel\n  static async fetchParallel (ipfs, hashes, { length, exclude = [], timeout, concurrency, onProgressCallback }) {\n    const fetchOne = async (hash) => EntryIO.fetchAll(ipfs, hash, { length, exclude, timeout, onProgressCallback, concurrency })\n    const concatArrays = (arr1, arr2) => arr1.concat(arr2)\n    const flatten = (arr) => arr.reduce(concatArrays, [])\n    const res = await pMap(hashes, fetchOne, { concurrency: Math.max(concurrency || hashes.length, 1) })\n    return flatten(res)\n  }\n\n  /**\n   * Fetch log entries\n   *\n   * @param {IPFS} [ipfs] An IPFS instance\n   * @param {string} [hash] Multihash of the entry to fetch\n   * @param {string} [parent] Parent of the node to be fetched\n   * @param {Object} [all] Entries to skip\n   * @param {Number} [amount=-1] How many entries to fetch\n   * @param {Number} [depth=0] Current depth of the recursion\n   * @param {function(hash, entry, parent, depth)} onProgressCallback\n   * @returns {Promise<Array<Entry>>}\n   */\n  static async fetchAll (ipfs, hashes, { length = -1, exclude = [], timeout, onProgressCallback, onStartProgressCallback, concurrency = 32, delay = 0 } = {}) {\n    let result = []\n    let cache = {}\n    let loadingCache = {}\n    let loadingQueue = Array.isArray(hashes)\n      ? { 0: hashes.slice() }\n      : { 0: [hashes] }\n    let running = 0 // keep track of how many entries are being fetched at any time\n    let maxClock = 0 // keep track of the latest clock time during load\n    let minClock = 0 // keep track of the minimum clock time during load\n\n    // Does the loading queue have more to process?\n    const loadingQueueHasMore = () => Object.values(loadingQueue).find(hasItems) !== undefined\n\n    // Add a multihash to the loading queue\n    const addToLoadingQueue = (e, idx) => {\n      if (!loadingCache[e]) {\n        if (!loadingQueue[idx]) loadingQueue[idx] = []\n        if (!loadingQueue[idx].includes(e)) {\n          loadingQueue[idx].push(e)\n        }\n        loadingCache[e] = true\n      }\n    }\n\n    // Get the next items to process from the loading queue\n    const getNextFromQueue = (length = 1) => {\n      const getNext = (res, key, idx) => {\n        const nextItems = loadingQueue[key]\n        while (nextItems.length > 0 && res.length < length) {\n          const hash = nextItems.shift()\n          res.push(hash)\n        }\n        if (nextItems.length === 0) {\n          delete loadingQueue[key]\n        }\n        return res\n      }\n      return Object.keys(loadingQueue).reduce(getNext, [])\n    }\n\n    // Add entries that we don't need to fetch to the \"cache\"\n    const addToExcludeCache = e => { cache[e.hash] = true }\n\n    // Fetch one entry and add it to the results\n    const fetchEntry = async (hash) => {\n      if (!hash || cache[hash]) {\n        return\n      }\n\n      return new Promise(async (resolve, reject) => {\n        // Resolve the promise after a timeout (if given) in order to\n        // not get stuck loading a block that is unreachable\n        const timer = timeout && timeout > 0\n          ? setTimeout(() => {\n            console.warn(`Warning: Couldn't fetch entry '${hash}', request timed out (${timeout}ms)`)\n            resolve()\n          }, timeout)\n          : null\n\n        const addToResults = (entry) => {\n          if (Entry.isEntry(entry)) {\n            const ts = entry.clock.time\n\n            // Update min/max clocks\n            maxClock = Math.max(maxClock, ts)\n            minClock = result.length > 0\n              ? Math.min(result[result.length - 1].clock.time, minClock)\n              : maxClock\n\n            const isLater = (result.length >= length && ts >= minClock)\n            const calculateIndex = (idx) => maxClock - ts + ((idx + 1) * idx)\n\n            // Add the entry to the results if\n            // 1) we're fetching all entries\n            // 2) results is not filled yet\n            // the clock of the entry is later than current known minimum clock time\n            if (length < 0 || result.length < length || isLater) {\n              result.push(entry)\n              cache[hash] = true\n\n              if (onProgressCallback) {\n                onProgressCallback(hash, entry, result.length, result.length)\n              }\n            }\n\n            if (length < 0) {\n              // If we're fetching all entries (length === -1), adds nexts and refs to the queue\n              entry.next.forEach(addToLoadingQueue)\n              if (entry.refs) entry.refs.forEach(addToLoadingQueue)\n            } else {\n              // If we're fetching entries up to certain length,\n              // fetch the next if result is filled up, to make sure we \"check\"\n              // the next entry if its clock is later than what we have in the result\n              if (result.length < length || ts > minClock || (ts === minClock && !cache[entry.hash])) {\n                entry.next.forEach(e => addToLoadingQueue(e, calculateIndex(0)))\n              }\n              if (entry.refs && (result.length + entry.refs.length <= length)) {\n                entry.refs.forEach((e, i) => addToLoadingQueue(e, calculateIndex(i)))\n              }\n            }\n          }\n        }\n\n        if (onStartProgressCallback) {\n          onStartProgressCallback(hash, null, 0, result.length)\n        }\n\n        try {\n          // Load the entry\n          const entry = await Entry.fromMultihash(ipfs, hash)\n\n          // Add it to the results\n          addToResults(entry)\n\n          // Simulate network latency (for debugging purposes)\n          if (delay > 0) {\n            const sleep = (ms = 0) => new Promise(resolve => setTimeout(resolve, ms))\n            await sleep(delay)\n          }\n          resolve()\n        } catch (e) {\n          reject(e)\n        } finally {\n          clearTimeout(timer)\n        }\n      })\n    }\n\n    // One loop of processing the loading queue\n    const _processQueue = async () => {\n      if (running < concurrency) {\n        const nexts = getNextFromQueue(concurrency)\n        running += nexts.length\n        await pMap(nexts, fetchEntry)\n        running -= nexts.length\n      }\n    }\n\n    // Add entries to exclude from processing to the cache before we start\n    exclude.forEach(addToExcludeCache)\n\n    // Fetch entries\n    await pDoWhilst(_processQueue, loadingQueueHasMore)\n\n    return result\n  }\n}\n\nmodule.exports = EntryIO\n"]},"metadata":{},"sourceType":"script"}