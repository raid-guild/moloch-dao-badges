{"ast":null,"code":"'use strict';\n\nconst Entry = require('./entry');\n\nconst EntryIO = require('./entry-io');\n\nconst Sorting = require('./log-sorting');\n\nconst {\n  LastWriteWins,\n  NoZeroes\n} = Sorting;\n\nconst LogError = require('./log-errors');\n\nconst {\n  isDefined,\n  findUniques,\n  difference,\n  io\n} = require('./utils');\n\nconst IPLD_LINKS = ['heads'];\n\nconst last = (arr, n) => arr.slice(arr.length - Math.min(arr.length, n), arr.length);\n\nclass LogIO {\n  //\n\n  /**\n   * Get the multihash of a Log.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Log} log Log to get a multihash for\n   * @returns {Promise<string>}\n   * @deprecated\n   */\n  static async toMultihash(ipfs, log, {\n    format\n  } = {}) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError();\n    if (!isDefined(log)) throw LogError.LogNotDefinedError();\n    if (!isDefined(format)) format = 'dag-cbor';\n    if (log.values.length < 1) throw new Error(`Can't serialize an empty log`);\n    return io.write(ipfs, format, log.toJSON(), {\n      links: IPLD_LINKS\n    });\n  }\n  /**\n   * Create a log from a hashes.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {string} hash The hash of the log\n   * @param {Object} options\n   * @param {number} options.length How many items to include in the log\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   */\n\n\n  static async fromMultihash(ipfs, hash, {\n    length = -1,\n    exclude = [],\n    timeout,\n    concurrency,\n    sortFn,\n    onProgressCallback\n  }) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError();\n    if (!isDefined(hash)) throw new Error(`Invalid hash: ${hash}`);\n    const logData = await io.read(ipfs, hash, {\n      links: IPLD_LINKS\n    });\n    if (!logData.heads || !logData.id) throw LogError.NotALogError(); // Use user provided sorting function or the default one\n\n    sortFn = sortFn || NoZeroes(LastWriteWins);\n\n    const isHead = e => logData.heads.includes(e.hash);\n\n    const all = await EntryIO.fetchAll(ipfs, logData.heads, {\n      length,\n      exclude,\n      timeout,\n      concurrency,\n      onProgressCallback\n    });\n    const logId = logData.id;\n    const entries = length > -1 ? last(all.sort(sortFn), length) : all;\n    const heads = entries.filter(isHead);\n    return {\n      logId,\n      entries,\n      heads\n    };\n  }\n  /**\n   * Create a log from an entry hash.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {string} hash The hash of the entry\n   * @param {Object} options\n   * @param {number} options.length How many items to include in the log\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   */\n\n\n  static async fromEntryHash(ipfs, hash, {\n    length = -1,\n    exclude = [],\n    timeout,\n    concurrency,\n    sortFn,\n    onProgressCallback\n  }) {\n    if (!isDefined(ipfs)) throw LogError.IpfsNotDefinedError();\n    if (!isDefined(hash)) throw new Error(\"'hash' must be defined\"); // Convert input hash(s) to an array\n\n    const hashes = Array.isArray(hash) ? hash : [hash]; // Fetch given length, return size at least the given input entries\n\n    length = length > -1 ? Math.max(length, 1) : length;\n    const all = await EntryIO.fetchParallel(ipfs, hashes, {\n      length,\n      exclude,\n      timeout,\n      concurrency,\n      onProgressCallback\n    }); // Cap the result at the right size by taking the last n entries,\n    // or if given length is -1, then take all\n\n    sortFn = sortFn || NoZeroes(LastWriteWins);\n    const entries = length > -1 ? last(all.sort(sortFn), length) : all;\n    return {\n      entries\n    };\n  }\n  /**\n   * Creates a log data from a JSON object, to be passed to a Log constructor\n   *\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {json} json A json object containing valid log data\n   * @param {Object} options\n   * @param {number} options.length How many entries to include\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   **/\n\n\n  static async fromJSON(ipfs, json, {\n    length = -1,\n    timeout,\n    concurrency,\n    onProgressCallback\n  }) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError();\n    const {\n      id,\n      heads\n    } = json;\n    const headHashes = heads.map(e => e.hash);\n    const all = await EntryIO.fetchParallel(ipfs, headHashes, {\n      length,\n      timeout,\n      concurrency,\n      onProgressCallback\n    });\n    const entries = all.sort(Entry.compare);\n    return {\n      logId: id,\n      entries,\n      heads\n    };\n  }\n  /**\n   * Create a new log starting from an entry.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Entry|Array<Entry>} sourceEntries An entry or an array of entries to fetch a log from\n   * @param {Object} options\n   * @param {number} options.length How many entries to include\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   */\n\n\n  static async fromEntry(ipfs, sourceEntries, {\n    length = -1,\n    exclude = [],\n    timeout,\n    concurrency,\n    onProgressCallback\n  }) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError();\n    if (!isDefined(sourceEntries)) throw new Error(\"'sourceEntries' must be defined\"); // Make sure we only have Entry objects as input\n\n    if (!Array.isArray(sourceEntries) && !Entry.isEntry(sourceEntries)) {\n      throw new Error(`'sourceEntries' argument must be an array of Entry instances or a single Entry`);\n    }\n\n    if (!Array.isArray(sourceEntries)) {\n      sourceEntries = [sourceEntries];\n    } // Fetch given length, return size at least the given input entries\n\n\n    length = length > -1 ? Math.max(length, sourceEntries.length) : length; // Make sure we pass hashes instead of objects to the fetcher function\n\n    const hashes = sourceEntries.map(e => e.hash); // Fetch the entries\n\n    const all = await EntryIO.fetchParallel(ipfs, hashes, {\n      length,\n      exclude,\n      timeout,\n      concurrency,\n      onProgressCallback\n    }); // Combine the fetches with the source entries and take only uniques\n\n    const combined = sourceEntries.concat(all).concat(exclude);\n    const uniques = findUniques(combined, 'hash').sort(Entry.compare); // Cap the result at the right size by taking the last n entries\n\n    const sliced = uniques.slice(length > -1 ? -length : -uniques.length); // Make sure that the given input entries are present in the result\n    // in order to not lose references\n\n    const missingSourceEntries = difference(sliced, sourceEntries, 'hash');\n\n    const replaceInFront = (a, withEntries) => {\n      var sliced = a.slice(withEntries.length, a.length);\n      return withEntries.concat(sliced);\n    }; // Add the input entries at the beginning of the array and remove\n    // as many elements from the array before inserting the original entries\n\n\n    const entries = replaceInFront(sliced, missingSourceEntries);\n    const logId = entries[entries.length - 1].id;\n    return {\n      logId,\n      entries\n    };\n  }\n\n}\n\nmodule.exports = LogIO;","map":{"version":3,"sources":["/home/dekan/Projects/raid-guild/dao-badges-web/node_modules/ipfs-log/src/log-io.js"],"names":["Entry","require","EntryIO","Sorting","LastWriteWins","NoZeroes","LogError","isDefined","findUniques","difference","io","IPLD_LINKS","last","arr","n","slice","length","Math","min","LogIO","toMultihash","ipfs","log","format","IPFSNotDefinedError","LogNotDefinedError","values","Error","write","toJSON","links","fromMultihash","hash","exclude","timeout","concurrency","sortFn","onProgressCallback","logData","read","heads","id","NotALogError","isHead","e","includes","all","fetchAll","logId","entries","sort","filter","fromEntryHash","IpfsNotDefinedError","hashes","Array","isArray","max","fetchParallel","fromJSON","json","headHashes","map","compare","fromEntry","sourceEntries","isEntry","combined","concat","uniques","sliced","missingSourceEntries","replaceInFront","a","withEntries","module","exports"],"mappings":"AAAA;;AAEA,MAAMA,KAAK,GAAGC,OAAO,CAAC,SAAD,CAArB;;AACA,MAAMC,OAAO,GAAGD,OAAO,CAAC,YAAD,CAAvB;;AACA,MAAME,OAAO,GAAGF,OAAO,CAAC,eAAD,CAAvB;;AACA,MAAM;AAAEG,EAAAA,aAAF;AAAiBC,EAAAA;AAAjB,IAA8BF,OAApC;;AACA,MAAMG,QAAQ,GAAGL,OAAO,CAAC,cAAD,CAAxB;;AACA,MAAM;AAAEM,EAAAA,SAAF;AAAaC,EAAAA,WAAb;AAA0BC,EAAAA,UAA1B;AAAsCC,EAAAA;AAAtC,IAA6CT,OAAO,CAAC,SAAD,CAA1D;;AAEA,MAAMU,UAAU,GAAG,CAAC,OAAD,CAAnB;;AACA,MAAMC,IAAI,GAAG,CAACC,GAAD,EAAMC,CAAN,KAAYD,GAAG,CAACE,KAAJ,CAAUF,GAAG,CAACG,MAAJ,GAAaC,IAAI,CAACC,GAAL,CAASL,GAAG,CAACG,MAAb,EAAqBF,CAArB,CAAvB,EAAgDD,GAAG,CAACG,MAApD,CAAzB;;AAEA,MAAMG,KAAN,CAAY;AACV;;AACA;;;;;;;AAOA,eAAaC,WAAb,CAA0BC,IAA1B,EAAgCC,GAAhC,EAAqC;AAAEC,IAAAA;AAAF,MAAa,EAAlD,EAAsD;AACpD,QAAI,CAAChB,SAAS,CAACc,IAAD,CAAd,EAAsB,MAAMf,QAAQ,CAACkB,mBAAT,EAAN;AACtB,QAAI,CAACjB,SAAS,CAACe,GAAD,CAAd,EAAqB,MAAMhB,QAAQ,CAACmB,kBAAT,EAAN;AACrB,QAAI,CAAClB,SAAS,CAACgB,MAAD,CAAd,EAAwBA,MAAM,GAAG,UAAT;AACxB,QAAID,GAAG,CAACI,MAAJ,CAAWV,MAAX,GAAoB,CAAxB,EAA2B,MAAM,IAAIW,KAAJ,CAAW,8BAAX,CAAN;AAE3B,WAAOjB,EAAE,CAACkB,KAAH,CAASP,IAAT,EAAeE,MAAf,EAAuBD,GAAG,CAACO,MAAJ,EAAvB,EAAqC;AAAEC,MAAAA,KAAK,EAAEnB;AAAT,KAArC,CAAP;AACD;AAED;;;;;;;;;;;AASA,eAAaoB,aAAb,CAA4BV,IAA5B,EAAkCW,IAAlC,EACE;AAAEhB,IAAAA,MAAM,GAAG,CAAC,CAAZ;AAAeiB,IAAAA,OAAO,GAAG,EAAzB;AAA6BC,IAAAA,OAA7B;AAAsCC,IAAAA,WAAtC;AAAmDC,IAAAA,MAAnD;AAA2DC,IAAAA;AAA3D,GADF,EACmF;AACjF,QAAI,CAAC9B,SAAS,CAACc,IAAD,CAAd,EAAsB,MAAMf,QAAQ,CAACkB,mBAAT,EAAN;AACtB,QAAI,CAACjB,SAAS,CAACyB,IAAD,CAAd,EAAsB,MAAM,IAAIL,KAAJ,CAAW,iBAAgBK,IAAK,EAAhC,CAAN;AAEtB,UAAMM,OAAO,GAAG,MAAM5B,EAAE,CAAC6B,IAAH,CAAQlB,IAAR,EAAcW,IAAd,EAAoB;AAAEF,MAAAA,KAAK,EAAEnB;AAAT,KAApB,CAAtB;AAEA,QAAI,CAAC2B,OAAO,CAACE,KAAT,IAAkB,CAACF,OAAO,CAACG,EAA/B,EAAmC,MAAMnC,QAAQ,CAACoC,YAAT,EAAN,CAN8C,CAQjF;;AACAN,IAAAA,MAAM,GAAGA,MAAM,IAAI/B,QAAQ,CAACD,aAAD,CAA3B;;AACA,UAAMuC,MAAM,GAAGC,CAAC,IAAIN,OAAO,CAACE,KAAR,CAAcK,QAAd,CAAuBD,CAAC,CAACZ,IAAzB,CAApB;;AAEA,UAAMc,GAAG,GAAG,MAAM5C,OAAO,CAAC6C,QAAR,CAAiB1B,IAAjB,EAAuBiB,OAAO,CAACE,KAA/B,EAChB;AAAExB,MAAAA,MAAF;AAAUiB,MAAAA,OAAV;AAAmBC,MAAAA,OAAnB;AAA4BC,MAAAA,WAA5B;AAAyCE,MAAAA;AAAzC,KADgB,CAAlB;AAGA,UAAMW,KAAK,GAAGV,OAAO,CAACG,EAAtB;AACA,UAAMQ,OAAO,GAAGjC,MAAM,GAAG,CAAC,CAAV,GAAcJ,IAAI,CAACkC,GAAG,CAACI,IAAJ,CAASd,MAAT,CAAD,EAAmBpB,MAAnB,CAAlB,GAA+C8B,GAA/D;AACA,UAAMN,KAAK,GAAGS,OAAO,CAACE,MAAR,CAAeR,MAAf,CAAd;AACA,WAAO;AAAEK,MAAAA,KAAF;AAASC,MAAAA,OAAT;AAAkBT,MAAAA;AAAlB,KAAP;AACD;AAED;;;;;;;;;;;AASA,eAAaY,aAAb,CAA4B/B,IAA5B,EAAkCW,IAAlC,EACE;AAAEhB,IAAAA,MAAM,GAAG,CAAC,CAAZ;AAAeiB,IAAAA,OAAO,GAAG,EAAzB;AAA6BC,IAAAA,OAA7B;AAAsCC,IAAAA,WAAtC;AAAmDC,IAAAA,MAAnD;AAA2DC,IAAAA;AAA3D,GADF,EACmF;AACjF,QAAI,CAAC9B,SAAS,CAACc,IAAD,CAAd,EAAsB,MAAMf,QAAQ,CAAC+C,mBAAT,EAAN;AACtB,QAAI,CAAC9C,SAAS,CAACyB,IAAD,CAAd,EAAsB,MAAM,IAAIL,KAAJ,CAAU,wBAAV,CAAN,CAF2D,CAGjF;;AACA,UAAM2B,MAAM,GAAGC,KAAK,CAACC,OAAN,CAAcxB,IAAd,IAAsBA,IAAtB,GAA6B,CAACA,IAAD,CAA5C,CAJiF,CAKjF;;AACAhB,IAAAA,MAAM,GAAGA,MAAM,GAAG,CAAC,CAAV,GAAcC,IAAI,CAACwC,GAAL,CAASzC,MAAT,EAAiB,CAAjB,CAAd,GAAoCA,MAA7C;AACA,UAAM8B,GAAG,GAAG,MAAM5C,OAAO,CAACwD,aAAR,CAAsBrC,IAAtB,EAA4BiC,MAA5B,EAChB;AAAEtC,MAAAA,MAAF;AAAUiB,MAAAA,OAAV;AAAmBC,MAAAA,OAAnB;AAA4BC,MAAAA,WAA5B;AAAyCE,MAAAA;AAAzC,KADgB,CAAlB,CAPiF,CASjF;AACA;;AACAD,IAAAA,MAAM,GAAGA,MAAM,IAAI/B,QAAQ,CAACD,aAAD,CAA3B;AACA,UAAM6C,OAAO,GAAGjC,MAAM,GAAG,CAAC,CAAV,GAAcJ,IAAI,CAACkC,GAAG,CAACI,IAAJ,CAASd,MAAT,CAAD,EAAmBpB,MAAnB,CAAlB,GAA+C8B,GAA/D;AACA,WAAO;AAAEG,MAAAA;AAAF,KAAP;AACD;AAED;;;;;;;;;;;AASA,eAAaU,QAAb,CAAuBtC,IAAvB,EAA6BuC,IAA7B,EAAmC;AAAE5C,IAAAA,MAAM,GAAG,CAAC,CAAZ;AAAekB,IAAAA,OAAf;AAAwBC,IAAAA,WAAxB;AAAqCE,IAAAA;AAArC,GAAnC,EAA8F;AAC5F,QAAI,CAAC9B,SAAS,CAACc,IAAD,CAAd,EAAsB,MAAMf,QAAQ,CAACkB,mBAAT,EAAN;AACtB,UAAM;AAAEiB,MAAAA,EAAF;AAAMD,MAAAA;AAAN,QAAgBoB,IAAtB;AACA,UAAMC,UAAU,GAAGrB,KAAK,CAACsB,GAAN,CAAUlB,CAAC,IAAIA,CAAC,CAACZ,IAAjB,CAAnB;AACA,UAAMc,GAAG,GAAG,MAAM5C,OAAO,CAACwD,aAAR,CAAsBrC,IAAtB,EAA4BwC,UAA5B,EAChB;AAAE7C,MAAAA,MAAF;AAAUkB,MAAAA,OAAV;AAAmBC,MAAAA,WAAnB;AAAgCE,MAAAA;AAAhC,KADgB,CAAlB;AAEA,UAAMY,OAAO,GAAGH,GAAG,CAACI,IAAJ,CAASlD,KAAK,CAAC+D,OAAf,CAAhB;AACA,WAAO;AAAEf,MAAAA,KAAK,EAAEP,EAAT;AAAaQ,MAAAA,OAAb;AAAsBT,MAAAA;AAAtB,KAAP;AACD;AAED;;;;;;;;;;;AASA,eAAawB,SAAb,CAAwB3C,IAAxB,EAA8B4C,aAA9B,EACE;AAAEjD,IAAAA,MAAM,GAAG,CAAC,CAAZ;AAAeiB,IAAAA,OAAO,GAAG,EAAzB;AAA6BC,IAAAA,OAA7B;AAAsCC,IAAAA,WAAtC;AAAmDE,IAAAA;AAAnD,GADF,EAC2E;AACzE,QAAI,CAAC9B,SAAS,CAACc,IAAD,CAAd,EAAsB,MAAMf,QAAQ,CAACkB,mBAAT,EAAN;AACtB,QAAI,CAACjB,SAAS,CAAC0D,aAAD,CAAd,EAA+B,MAAM,IAAItC,KAAJ,CAAU,iCAAV,CAAN,CAF0C,CAIzE;;AACA,QAAI,CAAC4B,KAAK,CAACC,OAAN,CAAcS,aAAd,CAAD,IAAiC,CAACjE,KAAK,CAACkE,OAAN,CAAcD,aAAd,CAAtC,EAAoE;AAClE,YAAM,IAAItC,KAAJ,CAAW,gFAAX,CAAN;AACD;;AAED,QAAI,CAAC4B,KAAK,CAACC,OAAN,CAAcS,aAAd,CAAL,EAAmC;AACjCA,MAAAA,aAAa,GAAG,CAACA,aAAD,CAAhB;AACD,KAXwE,CAazE;;;AACAjD,IAAAA,MAAM,GAAGA,MAAM,GAAG,CAAC,CAAV,GAAcC,IAAI,CAACwC,GAAL,CAASzC,MAAT,EAAiBiD,aAAa,CAACjD,MAA/B,CAAd,GAAuDA,MAAhE,CAdyE,CAgBzE;;AACA,UAAMsC,MAAM,GAAGW,aAAa,CAACH,GAAd,CAAkBlB,CAAC,IAAIA,CAAC,CAACZ,IAAzB,CAAf,CAjByE,CAmBzE;;AACA,UAAMc,GAAG,GAAG,MAAM5C,OAAO,CAACwD,aAAR,CAAsBrC,IAAtB,EAA4BiC,MAA5B,EAChB;AAAEtC,MAAAA,MAAF;AAAUiB,MAAAA,OAAV;AAAmBC,MAAAA,OAAnB;AAA4BC,MAAAA,WAA5B;AAAyCE,MAAAA;AAAzC,KADgB,CAAlB,CApByE,CAuBzE;;AACA,UAAM8B,QAAQ,GAAGF,aAAa,CAACG,MAAd,CAAqBtB,GAArB,EAA0BsB,MAA1B,CAAiCnC,OAAjC,CAAjB;AACA,UAAMoC,OAAO,GAAG7D,WAAW,CAAC2D,QAAD,EAAW,MAAX,CAAX,CAA8BjB,IAA9B,CAAmClD,KAAK,CAAC+D,OAAzC,CAAhB,CAzByE,CA2BzE;;AACA,UAAMO,MAAM,GAAGD,OAAO,CAACtD,KAAR,CAAcC,MAAM,GAAG,CAAC,CAAV,GAAc,CAACA,MAAf,GAAwB,CAACqD,OAAO,CAACrD,MAA/C,CAAf,CA5ByE,CA8BzE;AACA;;AACA,UAAMuD,oBAAoB,GAAG9D,UAAU,CAAC6D,MAAD,EAASL,aAAT,EAAwB,MAAxB,CAAvC;;AAEA,UAAMO,cAAc,GAAG,CAACC,CAAD,EAAIC,WAAJ,KAAoB;AACzC,UAAIJ,MAAM,GAAGG,CAAC,CAAC1D,KAAF,CAAQ2D,WAAW,CAAC1D,MAApB,EAA4ByD,CAAC,CAACzD,MAA9B,CAAb;AACA,aAAO0D,WAAW,CAACN,MAAZ,CAAmBE,MAAnB,CAAP;AACD,KAHD,CAlCyE,CAuCzE;AACA;;;AACA,UAAMrB,OAAO,GAAGuB,cAAc,CAACF,MAAD,EAASC,oBAAT,CAA9B;AACA,UAAMvB,KAAK,GAAGC,OAAO,CAACA,OAAO,CAACjC,MAAR,GAAiB,CAAlB,CAAP,CAA4ByB,EAA1C;AACA,WAAO;AAAEO,MAAAA,KAAF;AAASC,MAAAA;AAAT,KAAP;AACD;;AApJS;;AAuJZ0B,MAAM,CAACC,OAAP,GAAiBzD,KAAjB","sourcesContent":["'use strict'\n\nconst Entry = require('./entry')\nconst EntryIO = require('./entry-io')\nconst Sorting = require('./log-sorting')\nconst { LastWriteWins, NoZeroes } = Sorting\nconst LogError = require('./log-errors')\nconst { isDefined, findUniques, difference, io } = require('./utils')\n\nconst IPLD_LINKS = ['heads']\nconst last = (arr, n) => arr.slice(arr.length - Math.min(arr.length, n), arr.length)\n\nclass LogIO {\n  //\n  /**\n   * Get the multihash of a Log.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Log} log Log to get a multihash for\n   * @returns {Promise<string>}\n   * @deprecated\n   */\n  static async toMultihash (ipfs, log, { format } = {}) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError()\n    if (!isDefined(log)) throw LogError.LogNotDefinedError()\n    if (!isDefined(format)) format = 'dag-cbor'\n    if (log.values.length < 1) throw new Error(`Can't serialize an empty log`)\n\n    return io.write(ipfs, format, log.toJSON(), { links: IPLD_LINKS })\n  }\n\n  /**\n   * Create a log from a hashes.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {string} hash The hash of the log\n   * @param {Object} options\n   * @param {number} options.length How many items to include in the log\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   */\n  static async fromMultihash (ipfs, hash,\n    { length = -1, exclude = [], timeout, concurrency, sortFn, onProgressCallback }) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError()\n    if (!isDefined(hash)) throw new Error(`Invalid hash: ${hash}`)\n\n    const logData = await io.read(ipfs, hash, { links: IPLD_LINKS })\n\n    if (!logData.heads || !logData.id) throw LogError.NotALogError()\n\n    // Use user provided sorting function or the default one\n    sortFn = sortFn || NoZeroes(LastWriteWins)\n    const isHead = e => logData.heads.includes(e.hash)\n\n    const all = await EntryIO.fetchAll(ipfs, logData.heads,\n      { length, exclude, timeout, concurrency, onProgressCallback })\n\n    const logId = logData.id\n    const entries = length > -1 ? last(all.sort(sortFn), length) : all\n    const heads = entries.filter(isHead)\n    return { logId, entries, heads }\n  }\n\n  /**\n   * Create a log from an entry hash.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {string} hash The hash of the entry\n   * @param {Object} options\n   * @param {number} options.length How many items to include in the log\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   */\n  static async fromEntryHash (ipfs, hash,\n    { length = -1, exclude = [], timeout, concurrency, sortFn, onProgressCallback }) {\n    if (!isDefined(ipfs)) throw LogError.IpfsNotDefinedError()\n    if (!isDefined(hash)) throw new Error(\"'hash' must be defined\")\n    // Convert input hash(s) to an array\n    const hashes = Array.isArray(hash) ? hash : [hash]\n    // Fetch given length, return size at least the given input entries\n    length = length > -1 ? Math.max(length, 1) : length\n    const all = await EntryIO.fetchParallel(ipfs, hashes,\n      { length, exclude, timeout, concurrency, onProgressCallback })\n    // Cap the result at the right size by taking the last n entries,\n    // or if given length is -1, then take all\n    sortFn = sortFn || NoZeroes(LastWriteWins)\n    const entries = length > -1 ? last(all.sort(sortFn), length) : all\n    return { entries }\n  }\n\n  /**\n   * Creates a log data from a JSON object, to be passed to a Log constructor\n   *\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {json} json A json object containing valid log data\n   * @param {Object} options\n   * @param {number} options.length How many entries to include\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   **/\n  static async fromJSON (ipfs, json, { length = -1, timeout, concurrency, onProgressCallback }) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError()\n    const { id, heads } = json\n    const headHashes = heads.map(e => e.hash)\n    const all = await EntryIO.fetchParallel(ipfs, headHashes,\n      { length, timeout, concurrency, onProgressCallback })\n    const entries = all.sort(Entry.compare)\n    return { logId: id, entries, heads }\n  }\n\n  /**\n   * Create a new log starting from an entry.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Entry|Array<Entry>} sourceEntries An entry or an array of entries to fetch a log from\n   * @param {Object} options\n   * @param {number} options.length How many entries to include\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   */\n  static async fromEntry (ipfs, sourceEntries,\n    { length = -1, exclude = [], timeout, concurrency, onProgressCallback }) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError()\n    if (!isDefined(sourceEntries)) throw new Error(\"'sourceEntries' must be defined\")\n\n    // Make sure we only have Entry objects as input\n    if (!Array.isArray(sourceEntries) && !Entry.isEntry(sourceEntries)) {\n      throw new Error(`'sourceEntries' argument must be an array of Entry instances or a single Entry`)\n    }\n\n    if (!Array.isArray(sourceEntries)) {\n      sourceEntries = [sourceEntries]\n    }\n\n    // Fetch given length, return size at least the given input entries\n    length = length > -1 ? Math.max(length, sourceEntries.length) : length\n\n    // Make sure we pass hashes instead of objects to the fetcher function\n    const hashes = sourceEntries.map(e => e.hash)\n\n    // Fetch the entries\n    const all = await EntryIO.fetchParallel(ipfs, hashes,\n      { length, exclude, timeout, concurrency, onProgressCallback })\n\n    // Combine the fetches with the source entries and take only uniques\n    const combined = sourceEntries.concat(all).concat(exclude)\n    const uniques = findUniques(combined, 'hash').sort(Entry.compare)\n\n    // Cap the result at the right size by taking the last n entries\n    const sliced = uniques.slice(length > -1 ? -length : -uniques.length)\n\n    // Make sure that the given input entries are present in the result\n    // in order to not lose references\n    const missingSourceEntries = difference(sliced, sourceEntries, 'hash')\n\n    const replaceInFront = (a, withEntries) => {\n      var sliced = a.slice(withEntries.length, a.length)\n      return withEntries.concat(sliced)\n    }\n\n    // Add the input entries at the beginning of the array and remove\n    // as many elements from the array before inserting the original entries\n    const entries = replaceInFront(sliced, missingSourceEntries)\n    const logId = entries[entries.length - 1].id\n    return { logId, entries }\n  }\n}\n\nmodule.exports = LogIO\n"]},"metadata":{},"sourceType":"script"}